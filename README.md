<p align="center">
  <a href="https://www.python.org/"><img alt="Python" src="https://img.shields.io/badge/python-3.12+-3776AB?logo=python"></a>
  <a href="https://pytorch.org/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-2.5-E34F26?logo=pytorch"></a>
  <a href="https://streamlit.io/"><img alt="Streamlit" src="https://img.shields.io/badge/Streamlit-1.x-FF4B4B?logo=streamlit"></a>
  <a href="https://wandb.ai/"><img alt="Weights & Biases" src="https://img.shields.io/badge/W%26B-tracking-orange?logo=wandb"></a>
  <a href="https://opensource.org/licenses/MIT"><img alt="License: MIT" src="https://img.shields.io/badge/license-MIT-green.svg"></a>
</p>


# Clasificador de Entornos

Creaci√≥n de una **IA** capaz de recibir una imagen de entrada y clasificarla seg√∫n su entorno o contexto (por ejemplo, `Kitchen`, `Office`, `Bedroom`, etc.).  

Este proyecto **entrena** diferentes **Redes Neuronales Convolucionales (CNN)** a partir de **modelos preentrenados** (ResNet, ResNeXt, RegNet, etc.), y aprovecha **Weights & Biases (W&B)** para registrar las m√©tricas de entrenamiento de cada modelo.

## Descripci√≥n General

1. **Clases de entornos**  
   Se entren√≥ la IA para predecir m√∫ltiples ambientes. Los datos se encuentran en la carpeta `data`, organizada en subdirectorios para entrenamiento y validaci√≥n.

2. **App con Streamlit**  
   El proyecto incluye una **app con Streamlit** que, de forma amigable, permite subir una imagen y clasificarla usando cualquiera de los modelos disponibles.

3. **Monitoreo con Weights & Biases**  
   Todos los entrenamientos y sus m√©tricas se reportan a W&B, facilitando la comparaci√≥n de modelos, arquitecturas, √©pocas y otros hiperpar√°metros.

---

## üéØ Demo Online

Ya puedes probar nuestra app directamente desde el navegador sin necesidad de clonar ni configurar nada.

üìå Hemos desplegado una **demo funcional** con un modelo que alcanza aproximadamente **94% de accuracy** en validaci√≥n, entrenado con la arquitectura `resnext101_32x8d` y 13 capas descongeladas.

üîó **Accede aqu√≠**:
üëâ [https://environments-clasification.streamlit.app](https://environments-clasification.streamlit.app/)

1. Sube una imagen desde tu equipo.
2. Selecciona el modelo preentrenado.
3. Obt√©n la predicci√≥n del entorno (`Kitchen`, `Office`, `Bedroom`, etc.) al instante.

---

# Ejecuci√≥n Local Desde Cero

## 1. Clona el repositorio

```bash
git clone https://github.com/Adarve999/EnvironmentClasification-DeepLearning.git
cd EnvironmentClasification-DeepLearning
```

## 2. Crea un entorno virtual e instala las dependencias

```bash
conda env create -f env_dev.yaml
conda env activate dl_clasification
```

## 3 Entrenamiento o reutilizar modelos guardados en Google Drive

### 3.a Entrena un modelo

```bash
python src/train_model.py
```

1. Carga pesos preentrenados.
2. Entrena en `data/training`, validando en `data/validation`.
3. Reporta m√©tricas a W&B (si lo configuras) y guarda el mejor modelo en `models/`.

#### Opciones de entrenamiento

- **Learning Rate**: Se solicita en consola; por defecto 1e-4.  
- **Capas descongeladas**: Tambi√©n se ingresa en consola; por defecto 0.  
- **N√∫mero de √©pocas**: Si no se ingresa, por defecto 1.

### 3.b Coger un modelo preentrenado de la carpeta de Google Drive

Debido a que algunos de los modelos entrenados superan el l√≠mite de 100 MB impuesto por GitHub, se han subido a una carpeta de Google Drive para facilitar su descarga. Puedes acceder a todos los modelos en el siguiente enlace:

[Carpeta de Drive con los modelos](https://drive.google.com/drive/folders/1-9ZGXn6zHftPIvnO7t1JfCHSlHR5p2DT?usp=sharing)

Para utilizarlos:

1. Descarga los archivos `.pt` necesarios.
2. Col√≥calos en la carpeta `models/` de este repositorio.

## 4. Lanza la app de clasificaci√≥n con Streamlit

Una vez entrenado (o descargado) un modelo:

```bash
streamlit run src/app_DragAndDrop.py
```

1. Elige el modelo en el panel lateral.  
2. Sube la imagen.  
3. Se realiza la clasificaci√≥n y se muestra la clase predicha.

---

# Resultados y M√©tricas

La siguiente tabla resume algunos resultados de distintos modelos, con sus √©pocas, tasa de aprendizaje, capas descongeladas y exactitudes (Train y Validaci√≥n). Estos valores pueden variar seg√∫n el dataset y la configuraci√≥n final:

| Modelo              | √âpocas | Learning Rate | Unfrozen Layers | Accuracy (Train) | Accuracy (Validaci√≥n) |
|---------------------|-------:|--------------:|----------------:|-----------------:|-----------------------:|
| resnet50            |     50 | 0,0001        |               0 |           80,00% |                 80,00% |
| resnet50            |     30 | 0,0001        |               5 |           86,00% |                 87,00% |
| resnext101_32x8d    |    100 | 0,0001        |               0 |           84,00% |                 92,00% |
| resnext101_32x8d    |    200 | 0,0001        |               0 |           88,00% |                 92,30% |
| resnext101_32x8d    |     30 | 0,0001        |              13 |           93,30% |                 93,60% |
| resnext101_64x4d    |     30 | 0,0001        |               9 |           93,40% |                 94,20% |
| regnet_y_32gf       |     10 | 0,0001        |               0 |           71,00% |                 73,00% |
| regnet_y_32gf       |     10 | 0,0001        |               5 |           86,70% |                 94,70% |
| regnet_y_32gf       |     30 | 0,0001        |               7 |           91,90% |                 94,40% |

A partir de las m√©tricas presentadas en la tabla, se pueden extraer varias conclusiones:

1. **Eficacia de ResNeXt y RegNet:**  
   Los modelos **ResNeXt** (especialmente con configuraciones 32x8d y 64x4d) y **RegNet** (en configuraciones con m√°s capas descongeladas) suelen obtener mejores resultados de exactitud, tanto en entrenamiento como en validaci√≥n, superando en varios puntos porcentuales a ResNet50.

2. **Importancia de descongelar capas:**  
   Al comparar, por ejemplo, el modelo **resnet50** con 0 capas descongeladas (80%‚Äì80%) frente a **resnet50** con 5 capas descongeladas (86%‚Äì87%), se observa un aumento significativo en la exactitud. Esto indica que **ajustar capas internas** (unfreeze) puede permitir al modelo aprender mejor las caracter√≠sticas espec√≠ficas del dataset.

3. **Mayor n√∫mero de √©pocas no siempre significa mayor exactitud:**  
   En la familia **ResNeXt101_32x8d**, se ven resultados para 100 y 200 √©pocas con (casi) la misma validaci√≥n (~92%), mientras que con apenas 30 √©pocas y 13 capas descongeladas sube a ~93,6%. Esto sugiere que la **calidad del ajuste** (qu√© capas se descongelan y con qu√© LR) puede ser m√°s determinante que simplemente entrenar m√°s tiempo.

4. **Aprendizaje efectivo cuando se combina buena arquitectura + capas descongeladas:**  
   Modelos como **resnext101_64x4d** con 9 capas descongeladas logran un 93,4% en training y 94,2% en validaci√≥n, lo que demuestra la eficacia de descongelar parte de la red y elegir una arquitectura con alta cardinalidad.

5. **RegNet mejora significativamente con m√°s capas descongeladas y m√°s √©pocas:**  
   Pasar de 0 capas descongeladas (71%‚Äì73%) a 5 capas (86,7%‚Äì94,7%) o 7 capas (91,9%‚Äì94,4%) muestra un salto grande en rendimiento, confirmando que la estrategia de ajuste incide mucho en la exactitud final.

---

# Authors

- Rub√©n Adarve P√©rez
- Marta Rodr√≠guez Hebles
- Maria Valvanera Gil de Biedma
- Blanca Sayas Ladaga

Please use this bibtex if you want to cite this repository (main branch) in your publications:

```bibtex
@misc{EnvironmentClasification-DeepLearning,
  author       = {Rub√©n Adarve P√©rez, Marta Rodr√≠guez Hebles, Maria Valvanera Gil de Biedma, Blanca Sayas Ladaga},
  title        = {Deep Learning App: Clasificador de Entornos},
  year         = {2025},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/Adarve999/EnvironmentClasification-DeepLearning}},
}
```
